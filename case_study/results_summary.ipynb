{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_infractions(record):\n",
    "    infraction_counts = {\n",
    "        'collisions_layout': 0,\n",
    "        'collisions_pedestrian': 0,\n",
    "        'collisions_vehicle': 0,\n",
    "        'outside_route_lanes': 0,\n",
    "        'red_light': 0,\n",
    "        'route_dev': 0,\n",
    "        'route_timeout': 0,\n",
    "        'stop_infraction': 0,\n",
    "        'vehicle_blocked': 0\n",
    "    }\n",
    "    for key, value in record[\"infractions\"].items():\n",
    "        infraction_counts[key] += len(value)\n",
    "    return infraction_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_route_df(exp_path):\n",
    "    # Define the column names\n",
    "    columns = [\n",
    "        'model_name', 'model_number', 'run', 'route', 'score_composed', 'score_penalty', 'score_route',\n",
    "        'collisions_layout', 'collisions_pedestrian', 'collisions_vehicle', 'outside_route_lanes',\n",
    "        'red_light', 'route_dev', 'route_timeout', 'stop_infraction', 'vehicle_blocked'\n",
    "    ]\n",
    "\n",
    "    # Create an empty DataFrame with the specified columns\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    for run in os.listdir(exp_path):\n",
    "        for model in os.listdir(f\"{exp_path}/{run}\"):\n",
    "            model_name = model.split(\"_mn_\")[0]\n",
    "            model_number = model.split(\"_mn_\")[1]\n",
    "            results_path = f\"{exp_path}/{run}/{model}/results.json\"\n",
    "            # Read json file\n",
    "            with open(results_path) as f:\n",
    "                results = json.load(f)\n",
    "            if len(results[\"values\"]) == 0:\n",
    "                print(f\"No results in {results_path}\")\n",
    "                continue\n",
    "            for idx in range(10):\n",
    "                record = results['_checkpoint']['records'][idx]\n",
    "                infraction_counts = get_infractions(record)\n",
    "                scores = record[\"scores\"]\n",
    "                \n",
    "                row = [model_name, model_number, run, idx]\n",
    "                row.extend(list(scores.values()))\n",
    "                row.extend(list(infraction_counts.values()))\n",
    "                df.loc[len(df)] = row\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dist(stop_inf_no_pl, stop_inf_pl):\n",
    "    data = pd.DataFrame({\n",
    "        'No Pl': stop_inf_no_pl,\n",
    "        'Pl': stop_inf_pl\n",
    "    })\n",
    "    # Plotting the histograms\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(stop_inf_no_pl, kde=True, color='blue', label='No Pl', bins=10)\n",
    "    sns.histplot(stop_inf_pl, kde=True, color='red', label='Pl', bins=10)\n",
    "    plt.legend()\n",
    "    plt.title('Histogram of No Pl and Pl')\n",
    "\n",
    "    # Plotting the box plots\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(data=data)\n",
    "    plt.title('Box Plot of No Pl and Pl')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest(stop_inf_no_pl, stop_inf_pl, print_results=False):\n",
    "    t_statistic, p_value = ttest_ind(stop_inf_no_pl, stop_inf_pl)\n",
    "    data = pd.DataFrame({\n",
    "        'No Pl': stop_inf_no_pl,\n",
    "        'Pl': stop_inf_pl\n",
    "    })\n",
    "    aux = data.aggregate(['mean', 'std'])\n",
    "    improvement = (aux[\"No Pl\"][\"mean\"] - aux[\"Pl\"][\"mean\"]) / aux[\"No Pl\"][\"mean\"] * 100\n",
    "    \n",
    "    if print_results:\n",
    "        print(f\"t-statistic: {t_statistic}\")\n",
    "        print(f\"P-value: {p_value}\")\n",
    "        if p_value < 0.05:\n",
    "            print(\"There is a significant difference!\")\n",
    "        print(\"---\")\n",
    "        print(f\"No Pl: {aux['No Pl']['mean']}\")\n",
    "        print(f\"Pl: {aux['Pl']['mean']}\")\n",
    "        print(f\"Improvement: {improvement:.2f}%\")\n",
    "    \n",
    "    return t_statistic, p_value, improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\"score_composed\", \"collisions_layout\", \"collisions_pedestrian\", \"collisions_vehicle\", \"outside_route_lanes\", \"red_light\", \"route_dev\", \"route_timeout\", \"stop_infraction\", \"vehicle_blocked\"]\n",
    "avgs = {\n",
    "    \"pl\": {},\n",
    "    \"nopl\": {}\n",
    "}\n",
    "for att in attributes:\n",
    "    avgs[\"pl\"][att] = 0\n",
    "    avgs[\"nopl\"][att] = 0\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"Epochs\", \"\\\\approach\", *attributes])\n",
    "studies = [\"5e5_5e\", \"5e5_10e\", \"5e5_15e\"]\n",
    "base_path = \"./results_summary/\"\n",
    "for study in studies:\n",
    "    n_epochs = int(study.split(\"_\")[1].split(\"e\")[0])\n",
    "    model_df = {}\n",
    "    for model_type in [\"pl\", \"nopl\"]:\n",
    "        study_path = base_path + f\"final_{model_type}_5e5_{n_epochs}e\"\n",
    "        route_df = get_route_df(study_path)\n",
    "        model_df[model_type] = route_df.groupby(['model_name', 'model_number', 'run']).agg(['mean'])\n",
    "    row1 = [n_epochs, \"\\\\cmark\"]\n",
    "    row2 = [n_epochs, \"\\\\xmark\"]\n",
    "    for att in attributes:\n",
    "        pl_data = model_df[\"pl\"][att][\"mean\"].tolist()\n",
    "        nopl_data = model_df[\"nopl\"][att][\"mean\"].tolist()\n",
    "        statistics, p_value, improvement = ttest(nopl_data, pl_data)\n",
    "        s1 = f\"{model_df['pl'][att]['mean'].mean():.2f} $\\\\pm$ {model_df['pl'][att]['mean'].std():.2f}\"\n",
    "        avgs[\"pl\"][att] += model_df['pl'][att]['mean'].mean()\n",
    "        s2 = f\"{model_df['nopl'][att]['mean'].mean():.2f} $\\\\pm$ {model_df['nopl'][att]['mean'].std():.2f}\"\n",
    "        avgs[\"nopl\"][att] += model_df['nopl'][att]['mean'].mean()\n",
    "        if att == \"score_composed\":\n",
    "            improvement = improvement * -1\n",
    "        if improvement > 0:\n",
    "            s1 = \"\\\\textbf{\" + s1 + \"}\"\n",
    "        else:\n",
    "            s2 = \"\\\\textbf{\" + s2 + \"}\"                \n",
    "        if p_value < 0.05:\n",
    "            if improvement > 0:\n",
    "                s1 = \"\\\\underline{\" + s1 + \"}\"\n",
    "            else:\n",
    "                s2 = \"\\\\underline{\" + s2 + \"}\"\n",
    "        row1.append(s1)\n",
    "        row2.append(s2)\n",
    "    results_df.loc[len(results_df)] = row1\n",
    "    results_df.loc[len(results_df)] = row2\n",
    "for att in attributes:\n",
    "    avgs[\"pl\"][att] /= len(studies)\n",
    "    avgs[\"nopl\"][att] /= len(studies)\n",
    "# Summary\n",
    "s1 = [f\"{avgs['pl'][att]:.2f}\" for att in attributes]\n",
    "row1 = [\"Avg\", \"\\\\cmark\"] + [\"\\\\multicolumn{1}{c}{\" + s1[idx] + \"}\" for idx in range(len(attributes))]\n",
    "results_df.loc[len(results_df)] = row1\n",
    "s2 = [f\"{avgs['nopl'][att]:.2f}\" for att in attributes]\n",
    "row2 = [\"Avg\", \"\\\\xmark\"] + [\"\\\\multicolumn{1}{c}{\" + s2[idx] + \"}\" for idx in range(len(attributes))]\n",
    "results_df.loc[len(results_df)] = row2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.loc[:, (results_df != '0.00').any(axis=0)]\n",
    "results_df = results_df.reindex(columns=[\"Epochs\", \"\\\\approach\", \"stop_infraction\", \"score_composed\", \"collisions_vehicle\", \"red_light\", \"vehicle_blocked\", \"route_timeout\"])\n",
    "results_df.rename(columns={'score_composed': 'Avg. Driving Score','stop_infraction': 'Stop sign infractions' , 'collisions_vehicle': 'Collisions Vehicles', 'red_light': 'Red Light Infractions', 'vehicle_blocked': 'Agent Blocked', 'route_timeout': 'Route Timeout'}, inplace=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df.to_latex(index=False, escape=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Property Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_path = \"results_summary/final_pl_5e5_10e\"\n",
    "df_pl = get_route_df(exp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pl_scores = df_pl.groupby(['model_name', 'model_number', 'run']).agg(['mean'])[['score_composed', 'score_route', 'score_penalty']]\n",
    "# Remove the 'mean' row\n",
    "df_pl_scores = df_pl_scores.reset_index(level=0, drop=True)\n",
    "# Rename the columns\n",
    "df_pl_scores.columns = ['driving_score', 'route_completion', 'infraction_penalty']\n",
    "# Reset the index\n",
    "df_pl_scores.reset_index(inplace=True)\n",
    "# df_pl_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Property Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_path = \"results_summary/final_nopl_5e5_10e\"\n",
    "df_no_pl = get_route_df(exp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_pl_scores = df_no_pl.groupby(['model_name', 'model_number', 'run']).agg(['mean'])[['score_composed', 'score_route', 'score_penalty']]\n",
    "# Remove the 'mean' row\n",
    "df_no_pl_scores = df_no_pl_scores.reset_index(level=0, drop=True)\n",
    "# Rename the columns\n",
    "df_no_pl_scores.columns = ['driving_score', 'route_completion', 'infraction_penalty']\n",
    "# Reset the index\n",
    "df_no_pl_scores.reset_index(inplace=True)\n",
    "# df_no_pl_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Zero infractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_columns = df_no_pl.loc[:, (df_no_pl != 0).any(axis=0)]\n",
    "non_zero_columns = non_zero_columns.iloc[:, 7:].columns.tolist()\n",
    "non_zero_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for infraction_type in non_zero_columns:\n",
    "    # No Pl\n",
    "    inf_no_pl = df_no_pl.groupby(['model_name', 'model_number','run']).agg(['sum'])[[infraction_type]]\n",
    "    inf_no_pl = inf_no_pl[infraction_type]['sum'].to_list()\n",
    "    # Pl\n",
    "    inf_pl = df_pl.groupby(['model_name', 'model_number','run']).agg(['sum'])[[infraction_type]]\n",
    "    inf_pl = inf_pl[infraction_type]['sum'].to_list()\n",
    "    \n",
    "    statistics, p_value, improvement = ttest(inf_no_pl, inf_pl)\n",
    "    if p_value < 0.05:\n",
    "        results[infraction_type] = f\"Significant - {'Improve' if improvement > 0 else 'Worsen'} by {improvement:.2f}%\"\n",
    "    else:\n",
    "        results[infraction_type] = f\"Not Significant - {'Improve' if improvement > 0 else 'Worsen'} by {improvement:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(results, orient='index', columns=['Results'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infraction_type = \"stop_infraction\"\n",
    "\n",
    "# No Pl\n",
    "inf_no_pl = df_no_pl.groupby(['model_name', 'model_number','run']).agg(['sum'])[[infraction_type]]\n",
    "inf_no_pl = inf_no_pl[infraction_type]['sum'].to_list()\n",
    "# Pl\n",
    "inf_pl = df_pl.groupby(['model_name', 'model_number','run']).agg(['sum'])[[infraction_type]]\n",
    "inf_pl = inf_pl[infraction_type]['sum'].to_list()\n",
    "\n",
    "ttest(inf_no_pl, inf_pl, print_results=True)\n",
    "plot_dist(inf_no_pl, inf_pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carla Avg Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Pl\n",
    "no_pl_score = df_no_pl_scores.groupby(['model_number','run']).agg(['first'])[['driving_score']]\n",
    "no_pl_score = no_pl_score['driving_score']['first'].to_list()\n",
    "# Pl\n",
    "pl_score = df_pl_scores.groupby(['model_number','run']).agg(['first'])[['driving_score']]\n",
    "pl_score = pl_score['driving_score']['first'].to_list()\n",
    "\n",
    "ttest(no_pl_score, pl_score, print_results=True)\n",
    "plot_dist(no_pl_score, pl_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TCP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
